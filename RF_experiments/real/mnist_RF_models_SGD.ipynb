{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run MLP (first layer weights fixed) on mnist and compute bias and variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import queue\n",
    "import os\n",
    "import sys\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.optim as optim\n",
    "import copy \n",
    "import pandas as pd\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, p, d, o):\n",
    "        \"\"\"RF_models\n",
    "        \n",
    "        Args:\n",
    "            p (int): the hidden size\n",
    "            d (int): the input feature dimension\n",
    "            o (int): the output dimension\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(d, p, bias=False)\n",
    "        self.fc2 = nn.Linear(p, o, bias=False)\n",
    "        self.p = p\n",
    "        self.d = d\n",
    "        self.o = o \n",
    "    def forward(self, x):\n",
    "        o = F.relu(self.fc1(x))\n",
    "        o = self.fc2(o)\n",
    "        return o\n",
    "class Ensemble_Two_Layer_NN(object):\n",
    "    def __init__(self, n_classifiers, p, d=784, o=10):\n",
    "        \"\"\"Ensemble_Two_Layer_NN\n",
    "        \n",
    "        Args:\n",
    "            p (int): the hidden size\n",
    "            d (int, optional): the input feature dimension\n",
    "            o (int, optional): the output dimension\n",
    "            coef (float, optional): the ridge regression penalty coefficient\n",
    "        \"\"\"\n",
    "        self.n_classifiers = n_classifiers\n",
    "        self.p = p\n",
    "        self.d = d \n",
    "        self.o = o \n",
    "        self.coef = coef\n",
    "        self.learners = queue.LifoQueue(maxsize = self.n_classifiers)\n",
    "        self.MODEL_TYPE = MLP\n",
    "    def __len__(self):\n",
    "        return len(self.learners.queue)\n",
    "    \n",
    "    def put_model_rho(self, model, rho):\n",
    "        self.learners.put([model, rho])\n",
    "    def get_init_model(self, cuda=True):\n",
    "        model = self.MODEL_TYPE(self.p, self.d, self.o)\n",
    "        if cuda:\n",
    "            model.cuda()\n",
    "        return model\n",
    "    def cuda(self):\n",
    "        if len(self) == 0:\n",
    "            return \n",
    "        else:\n",
    "            for model, rho in self.learners.queue:\n",
    "                model.cuda()\n",
    "            return\n",
    "    def train(self):\n",
    "        if len(self)!=0:\n",
    "            for model, rho in self.learners.queue:\n",
    "                model.train()\n",
    "    def eval(self):\n",
    "        if len(self)!=0:\n",
    "            for model, rho in self.learners.queue:\n",
    "                model.eval()\n",
    "    def forward(self, x):\n",
    "        Bs = x.size(0)\n",
    "        if len(self) == 0:\n",
    "            zeros = torch.zeros(Bs, self.o)\n",
    "            zeros = zeros.to(x.device)\n",
    "            return zeros\n",
    "        else:\n",
    "            outputs = torch.zeros(Bs, self.o)\n",
    "            outputs = outputs.to(x.device) \n",
    "            for model, rho in self.learners.queue:\n",
    "                output = model(x)\n",
    "                outputs += rho*output\n",
    "            return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subsample_dataset(trainset, subset):\n",
    "    trainsubset = copy.deepcopy(trainset)\n",
    "    trainsubset.data = [trainsubset.data[index] for index in subset]\n",
    "    trainsubset.targets = [trainsubset.targets[index] for index in subset]\n",
    "    return trainsubset\n",
    "def fix_width_number(width, n_classifiers):\n",
    "    return max(1, width//n_classifiers)\n",
    "\n",
    "# Training\n",
    "def train(net, trainset, permute_index, train_size, num_iters, lr, batch_size, coef):\n",
    "    net.train()\n",
    "    subsample_indexes = np.random.choice(permute_index, size=train_size)\n",
    "    trainsubset = get_subsample_dataset(trainset, subsample_indexes)\n",
    "    trainloader = torch.utils.data.DataLoader(trainsubset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for i_c in range(net.n_classifiers):\n",
    "        i_iter = 0\n",
    "        model = net.get_init_model(cuda=True)\n",
    "        rho = 1/net.n_classifiers\n",
    "        optimizer = torch.optim.SGD(model.fc2.parameters(), lr=lr)\n",
    "        lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size = num_iters//3, gamma = 0.1)\n",
    "        while i_iter < num_iters:\n",
    "            for inputs, targets in trainloader:\n",
    "                Bs = inputs.size(0)\n",
    "                inputs = inputs.reshape(Bs, -1)\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "                targets_onehot = torch.FloatTensor(targets.size(0), net.o).cuda()\n",
    "                targets_onehot.zero_()\n",
    "                targets_onehot.scatter_(1, targets.view(-1, 1).long(), 1)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets_onehot)\n",
    "                l2_reg = 0\n",
    "                for param in model.fc2.parameters():\n",
    "                    l2_reg += coef * torch.norm(param)\n",
    "                mse_loss = loss.item()\n",
    "                l2_loss = l2_reg.item()\n",
    "                loss += l2_reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                string = \"Train {} model: Iters [{}/{}] mse: {:.4f}, l2_loss: {:.4f}, train_loss:{:.4f}\".format(i_c+1, i_iter, num_iters,  mse_loss, l2_loss, loss.item())\n",
    "                sys.stdout.write(string+\"\\r\")\n",
    "                sys.stdout.flush()\n",
    "                i_iter +=1\n",
    "        net.put_model_rho(model, rho)\n",
    "    # after all models were trained, estimate the mse error\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in trainloader:\n",
    "        Bs = inputs.size(0)\n",
    "        inputs = inputs.reshape(Bs, -1)\n",
    "        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "        targets_onehot = torch.FloatTensor(targets.size(0), net.o).cuda()\n",
    "        targets_onehot.zero_()\n",
    "        targets_onehot.scatter_(1, targets.view(-1, 1).long(), 1)\n",
    "        outputs = net.forward(inputs)\n",
    "        loss = criterion(outputs, targets_onehot)\n",
    "        train_loss = loss.item() * outputs.numel()\n",
    "        _, predicted = outputs.max(1)\n",
    "        correct = predicted.eq(targets).sum().item()\n",
    "        total = targets.size(0)\n",
    "    return train_loss/ total , 100. * correct / total\n",
    "\n",
    "# Test\n",
    "def test(net, testloader):\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            Bs = inputs.size(0)\n",
    "            inputs = inputs.reshape(Bs, -1)\n",
    "            targets_onehot = torch.FloatTensor(targets.size(0), net.o).cuda()\n",
    "            targets_onehot.zero_()\n",
    "            targets_onehot.scatter_(1, targets.view(-1, 1).long(), 1)\n",
    "            outputs = net.forward(inputs)\n",
    "            loss = criterion(outputs, targets_onehot)\n",
    "            test_loss += loss.item() * outputs.numel()\n",
    "            _, predicted = outputs.max(1)\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return test_loss / total, 100. * correct / total\n",
    "\n",
    "def compute_bias_variance(net, testloader, trial, OUTPUST_SUM, OUTPUTS_SUMNORMSQUARED):\n",
    "    net.eval()\n",
    "    bias2 = 0\n",
    "    variance = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            inputs, targets = inputs.cuda(), targets.cuda()\n",
    "            Bs = inputs.size(0)\n",
    "            inputs = inputs.reshape(Bs, -1)\n",
    "            targets_onehot = torch.FloatTensor(targets.size(0), net.o).cuda()\n",
    "            targets_onehot.zero_()\n",
    "            targets_onehot.scatter_(1, targets.view(-1, 1).long(), 1)\n",
    "            outputs = net.forward(inputs)\n",
    "            OUTPUST_SUM[total:(total + targets.size(0)), :] += outputs\n",
    "            OUTPUTS_SUMNORMSQUARED[total:total + targets.size(0)] += outputs.norm(dim=1) ** 2.0\n",
    "\n",
    "            bias2 += (OUTPUST_SUM[total:total + targets.size(0), :] / (trial + 1) - targets_onehot).norm() ** 2.0\n",
    "            variance += OUTPUTS_SUMNORMSQUARED[total:total + targets.size(0)].sum()/(trial + 1) - (OUTPUST_SUM[total:total + targets.size(0), :]/(trial + 1)).norm() ** 2.0\n",
    "            total += targets.size(0)\n",
    "\n",
    "    return bias2 / total, variance / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "trainset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform_train)\n",
    "testset = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=transform_test)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=4)\n",
    "# loss definition\n",
    "criterion = nn.MSELoss(reduction='mean').cuda()\n",
    "\n",
    "def run_exps_sgd(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef,\n",
    "             outdir, save_csv, num_iters, lr, batch_size_list, K = 1):\n",
    "    df = pd.DataFrame()\n",
    "    for batch_size in batch_size_list:\n",
    "        for train_size in train_sizes:\n",
    "            hidden_sizes = P_Ns * train_size\n",
    "            hidden_sizes = np.unique([int(np.around(x)) for x in hidden_sizes])\n",
    "            for hidden_size in hidden_sizes:\n",
    "                TRAIN_ACC_SUM = 0.0\n",
    "                TEST_ACC_SUM = 0.0\n",
    "                TRAIN_LOSS_SUM = 0.0\n",
    "                TEST_LOSS_SUM = 0.0\n",
    "                permute_index = np.random.permutation(len(trainset))\n",
    "                OUTPUST_SUM = torch.Tensor(test_size, num_classes).zero_().cuda()\n",
    "                OUTPUTS_SUMNORMSQUARED = torch.Tensor(test_size).zero_().cuda()\n",
    "                for trial in range(num_trials):\n",
    "                    net = Ensemble_Two_Layer_NN(n_classifiers = K, p = fix_width_number(hidden_size, K), d=feature_dim, o=num_classes)\n",
    "                    net.cuda()\n",
    "                    train_loss, train_acc = train(net, trainset, permute_index, train_size,\n",
    "                                                 num_iters, lr, batch_size, coef)\n",
    "                    test_loss, test_acc = test(net, testloader)\n",
    "\n",
    "                    TRAIN_LOSS_SUM += train_loss\n",
    "                    TEST_LOSS_SUM += test_loss\n",
    "                    TRAIN_ACC_SUM += train_acc\n",
    "                    TEST_ACC_SUM += test_acc\n",
    "\n",
    "                    # compute bias and variance\n",
    "                    bias2, variance = compute_bias_variance(net, testloader, trial, OUTPUST_SUM, OUTPUTS_SUMNORMSQUARED)\n",
    "                    variance_unbias = variance * num_trials / (num_trials - 1.0)\n",
    "                    bias2_unbias = TEST_LOSS_SUM / (trial + 1) - variance_unbias\n",
    "                    print('Train size: [{}] hidden size: [{}] batch size: [{}] trial: {}, train_loss: {:.6f}, train acc: {}, test loss: {:.6f}, test acc: {}, bias2: {}, variance: {}'.format(\n",
    "                        train_size, hidden_size, batch_size,\n",
    "                        trial, TRAIN_LOSS_SUM / (trial + 1), TRAIN_ACC_SUM / (trial + 1), TEST_LOSS_SUM / (trial + 1),\n",
    "                        TEST_ACC_SUM / (trial + 1), bias2_unbias, variance_unbias))\n",
    "                    torch.cuda.empty_cache()\n",
    "                print('#'*50)\n",
    "                df = df.append({'train_size': train_size, 'hidden_size':hidden_size, 'batch_size': batch_size,\n",
    "                                'train_loss': TRAIN_LOSS_SUM / (trial + 1), 'train_acc': TRAIN_ACC_SUM / (trial + 1),\n",
    "                                'test_loss': TEST_LOSS_SUM / (trial + 1), 'test_acc': TEST_ACC_SUM / (trial + 1), \n",
    "                               'variance': variance_unbias.item(),\n",
    "                               'bias2': bias2_unbias.item()}, ignore_index=True)\n",
    "                df.to_csv(os.path.join(outdir, save_csv))\n",
    "    df.to_csv(os.path.join(outdir, save_csv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mnist_SGD/num_iters_500_coef=0.0001\n",
      "Train size: [784] hidden size: [8] batch size: [10] trial: 0, train_loss: 1.136294, train acc: 50.0, test loss: 1.374105, test acc: 9.57, bias2: 1.3741050958633423, variance: 5.838822203507732e-10\n",
      "Train size: [784] hidden size: [8] batch size: [10] trial: 1, train_loss: 1.194225, train acc: 25.0, test loss: 1.341597, test acc: 11.01, bias2: 1.1910598278045654, variance: 0.15053719282150269\n",
      "Train size: [784] hidden size: [8] batch size: [10] trial: 2, train_loss: 1.210376, train acc: 25.0, test loss: 1.362627, test acc: 11.99, bias2: 1.1203193664550781, variance: 0.24230729043483734\n",
      "Train size: [784] hidden size: [8] batch size: [10] trial: 3, train_loss: 1.194897, train acc: 18.75, test loss: 1.351751, test acc: 11.334999999999999, bias2: 1.0921508073806763, variance: 0.259600430727005\n",
      "Train size: [784] hidden size: [8] batch size: [10] trial: 4, train_loss: 1.184570, train acc: 25.0, test loss: 1.340106, test acc: 11.657999999999998, bias2: 1.0540608167648315, variance: 0.2860450744628906\n",
      "Train 1 model: Iters [309/500] mse: 0.1074, l2_loss: 0.0002, train_loss:0.1075\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-f790c785e4ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     run_exps_sgd(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef,\n\u001b[0;32m---> 18\u001b[0;31m              outdir, 'singleNN_output.csv', num_iters = num_iters, lr=lr, batch_size_list=batch_size_list,  K = 1)\n\u001b[0m\u001b[1;32m     19\u001b[0m     run_exps_sgd(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef,\n\u001b[1;32m     20\u001b[0m                  outdir, 'ensembleNNK=2_output.csv', num_iters = num_iters, lr=lr, batch_size_list=batch_size_list,  K = 2)\n",
      "\u001b[0;32m<ipython-input-7-5b19548d451d>\u001b[0m in \u001b[0;36mrun_exps_sgd\u001b[0;34m(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef, outdir, save_csv, num_iters, lr, batch_size_list, K)\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     train_loss, train_acc = train(net, trainset, permute_index, train_size,\n\u001b[0;32m---> 34\u001b[0;31m                                                  num_iters, lr, batch_size, coef)\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-f89725c502f7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, trainset, permute_index, train_size, num_iters, lr, batch_size, coef)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mlr_scheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlr_scheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStepLR\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mi_iter\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mnum_iters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrainloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                 \u001b[0mBs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \"\"\"\n\u001b[0;32m---> 92\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m255\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mByteStorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetbands\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mtobytes\u001b[0;34m(self, encoder_name, *args)\u001b[0m\n\u001b[1;32m    731\u001b[0m             \u001b[0margs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    732\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    734\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0;31m# unpack data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py35/lib/python3.5/site-packages/PIL/Image.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0;34m:\u001b[0m\u001b[0mrtype\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPixelAccess\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;32mclass\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mPIL\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPyAccess\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    819\u001b[0m         \"\"\"\n\u001b[0;32m--> 820\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirty\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    821\u001b[0m             \u001b[0;31m# realize palette\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    822\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mputpalette\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpalette\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_classes = 10\n",
    "num_trials = 50\n",
    "coef = 0.0001\n",
    "N_Ds = [1]\n",
    "feature_dim = 784\n",
    "lr = 0.01 \n",
    "batch_size_list = [1, 784]\n",
    "train_sizes = [int(np.around(x*feature_dim)) for x in N_Ds]\n",
    "test_size = 10000\n",
    "P_Ns = 10** np.linspace(-2, 1, 50)\n",
    "\n",
    "for num_iters in [500, 5000]:\n",
    "    outdir = 'mnist_SGD/num_iters_{}_coef={}'.format(num_iters, coef)\n",
    "    print(outdir)\n",
    "    if not os.path.exists(outdir):\n",
    "        os.makedirs(outdir)\n",
    "    run_exps_sgd(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef,\n",
    "             outdir, 'singleNN_output.csv', num_iters = num_iters, lr=lr, batch_size_list=batch_size_list,  K = 1)\n",
    "    run_exps_sgd(train_sizes, N_Ds, P_Ns, trainset, test_size, feature_dim, num_classes, num_trials, coef,\n",
    "                 outdir, 'ensembleNNK=2_output.csv', num_iters = num_iters, lr=lr, batch_size_list=batch_size_list,  K = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "font = {\n",
    "        'size'   : 18}\n",
    "matplotlib.rc('font', **font)\n",
    "figsize = (16, 5)\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "import pandas as pd\n",
    "\n",
    "def plot_bias_var(df, N_D, ymin=0, ymax=1.0):\n",
    "    fig1, axes1 = plt.subplots(1, 3, figsize=figsize)\n",
    "    axes1[0].set_xscale('log')\n",
    "    axes1[1].set_xscale('log')\n",
    "    axes1[2].set_xscale('log')\n",
    "    cur_df = df[df['train_size']/feature_dim==N_D]\n",
    "    test_loss = cur_df['test_loss']\n",
    "    bias2 = cur_df['bias2']\n",
    "    var = cur_df['variance']\n",
    "    P_N = cur_df['hidden_size']/cur_df['train_size']\n",
    "    axes1[0].plot(P_N, test_loss)\n",
    "    axes1[0].set_xlabel(\"P/N\")\n",
    "    axes1[0].set_ylabel(\"Test Loss\")\n",
    "    axes1[0].set_ylim(ymin, ymax)\n",
    "    axes1[1].plot(P_N, bias2)\n",
    "    axes1[1].set_xlabel(\"P/N\")\n",
    "    axes1[1].set_ylabel(\"Bias Square\")\n",
    "    axes1[1].set_ylim(ymin, ymax)\n",
    "    axes1[2].plot(P_N, var)\n",
    "    axes1[2].set_xlabel(\"P/N\")\n",
    "    axes1[2].set_ylabel(\"Variance\")\n",
    "    axes1[2].set_ylim(ymin, ymax)\n",
    "    fig1.suptitle(\"Bias-Variance Decomposition (N/D={:.2f})\".format(N_D))\n",
    "    plt.show()\n",
    "def plot_single_vs_ensemble(dfs_list, Ks_list, N_D, feature_dim, ymin=0, ymax=1.0):\n",
    "    assert len(dfs_list) == len(Ks_list)\n",
    "    fig1, axes1 = plt.subplots(1, 3, figsize=figsize)\n",
    "    for i in range(3):\n",
    "        axes1[i].set_xscale('log')\n",
    "    dfs_list = [df[df['train_size']/feature_dim==N_D] for df in dfs_list]\n",
    "    for cur_df, K in zip(dfs_list, Ks_list):\n",
    "        test_loss = cur_df['test_loss']\n",
    "        bias2 = cur_df['bias2']\n",
    "        var = cur_df['variance']\n",
    "        P_N = cur_df['hidden_size']/cur_df['train_size']\n",
    "        axes1[0].plot(P_N, test_loss, label='K={}'.format(K))\n",
    "        axes1[1].plot(P_N, bias2, label='K={}'.format(K))\n",
    "        axes1[2].plot(P_N, var, label='K={}'.format(K))\n",
    "    \n",
    "    axes1[0].set_xlabel(\"P/N\")\n",
    "    axes1[0].set_ylabel(\"Test Loss\")\n",
    "    axes1[0].set_ylim(ymin, ymax)\n",
    "    \n",
    "    axes1[1].set_xlabel(\"P/N\")\n",
    "    axes1[1].set_ylabel(\"Bias Square\")\n",
    "    axes1[1].set_ylim(ymin, ymax)\n",
    "    \n",
    "    axes1[2].set_xlabel(\"P/N\")\n",
    "    axes1[2].set_ylabel(\"Variance\")\n",
    "    axes1[2].set_ylim(ymin, ymax)\n",
    "    fig1.suptitle(\"Bias-Variance Decomposition (N/D={:.2f})\".format(N_D))\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K2_df = pd.read_csv(os.path.join(outdir, 'ensembleNNK=2_output.csv'))\n",
    "# K1_df = pd.read_csv(os.path.join(outdir, 'singleNN_output.csv'))\n",
    "# plot_single_vs_ensemble([K1_df, K2_df], [1, 2], N_Ds[0], 784,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
